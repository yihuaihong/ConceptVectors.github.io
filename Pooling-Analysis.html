<!doctype html><html lang="en" class="no-js"><head><meta name="google-site-verification" content="dSs91yaMZFstcVLxzwBo4NGnkZcLMDgV4LDPJ13WFjE"><meta name="google-site-verification" content="IdeLCPxHTXJ0OOEM05vk7UAv72xQh5b7NFrEZAYcJH4"> <!-- dSs91yaMZFstcVLxzwBo4NGnkZcLMDgV4LDPJ13WFjE --><meta charset="utf-8"> <!-- begin SEO --><title> Why and when should you pool? Analyzing Pooling in Recurrent Architectures Pratyush Maini</title><meta property="og:locale" content="en-US"><meta property="og:site_name" content="Pratyush Maini"><meta property="og:title" content="Why and when should you pool? Analyzing Pooling in Recurrent Architectures"><link rel="canonical" href="https://pratyushmaini.github.io/Pooling-Analysis/"><meta property="og:url" content="https://pratyushmaini.github.io/Pooling-Analysis/"><meta property="og:description" content=" Paper &raquo; Code &raquo; "><meta property="og:type" content="article"><meta property="article:published_time" content="2020-11-16T00:00:00-08:00"> <script type="application/ld+json"> { "@context" : "http://schema.org", "@type" : "Person", "name" : "Pratyush Maini", "url" : "https://pratyushmaini.github.io", "sameAs" : null } </script><meta name="google-site-verification" content="IdeLCPxHTXJ0OOEM05vk7UAv72xQh5b7NFrEZAYcJH4"> <!-- end SEO --><link href="https://pratyushmaini.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Pratyush Maini Feed"> <!-- http://t.co/dKP3o1e --><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="google-site-verification" content="dSs91yaMZFstcVLxzwBo4NGnkZcLMDgV4LDPJ13WFjE"><meta name="google-site-verification" content="IdeLCPxHTXJ0OOEM05vk7UAv72xQh5b7NFrEZAYcJH4"> <script> document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js '; </script> <!-- For all browsers --><link rel="stylesheet" href="static/css/main.css"><meta http-equiv="cleartype" content="on"> <!-- start custom head snippets --><link rel="apple-touch-icon" sizes="57x57" href="https://pratyushmaini.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="60x60" href="https://pratyushmaini.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="72x72" href="https://pratyushmaini.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="76x76" href="https://pratyushmaini.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="114x114" href="https://pratyushmaini.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="120x120" href="https://pratyushmaini.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="144x144" href="https://pratyushmaini.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="152x152" href="https://pratyushmaini.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="180x180" href="https://pratyushmaini.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ"><link rel="icon" type="image/png" href="https://pratyushmaini.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32"><link rel="icon" type="image/png" href="https://pratyushmaini.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192"><link rel="icon" type="image/png" href="https://pratyushmaini.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96"><link rel="icon" type="image/png" href="https://pratyushmaini.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16"><link rel="manifest" href="static/css/manifest.json"><link rel="mask-icon" href="https://pratyushmaini.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000"><link rel="shortcut icon" href="https://pratyushmaini.github.io/images/favicon.ico?v=M44lzPylqQ"><meta name="msapplication-TileColor" content="#000000"><meta name="msapplication-TileImage" content="https://pratyushmaini.github.io/images/mstile-144x144.png?v=M44lzPylqQ"><meta name="msapplication-config" content="https://pratyushmaini.github.io/images/browserconfig.xml?v=M44lzPylqQ"><meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="static/css/academicons.css"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } }); </script> <script src='static/js/latest.js' async=""></script> <!-- end custom head snippets --></head><body> <!--[if lt IE 9]><div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]--><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav"> <button><div class="navicon"></div></button><ul class="visible-links"><li class="masthead__menu-item masthead__menu-item--lg"><a href="index1.html">Pratyush Maini</a></li><li class="masthead__menu-item"><a href="blog.html">Blog</a></li><li class="masthead__menu-item"><a href="teaching.html">Teaching</a></li><li class="masthead__menu-item"><a href="extra.html">Not Research</a></li></ul><ul class="hidden-links hidden"></ul></nav></div></div></div><div id="main" role="main"><article class="page" itemscope="" itemtype="http://schema.org/CreativeWork"><meta itemprop="headline" content="Why and when should you pool? Analyzing Pooling in Recurrent Architectures"><meta itemprop="description" content=" Paper &raquo; Code &raquo; "><meta itemprop="datePublished" content="November 16, 2020"><div class="page__inner-wrap"><header><h1 class="page__title" itemprop="headline">Why and when should you pool? <br> Analyzing Pooling in Recurrent Architectures</h1><p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 9 minute read</p><p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-11-16T00:00:00-08:00">November 16, 2020</time></p></header><section class="page__content" itemprop="text"><div> <a class="btn btn-lg btn-warning" href="https://arxiv.org/abs/2005.00159" target="_blank" role="button">Paper &raquo; </a> <a class="btn btn-lg btn-secondary" href="https://github.com/dair-iitd/PoolingAnalysis" target="_blank" role="button">Code &raquo;</a> <br></div><style> .authorbox { /* height: 128px; */ /* padding: 100px; */ } .authorbox > div { text-align: center; /* width: 128px; */ display: inline-block; } .authorbox p { margin: 0; } .authorbox a{ color: #7e4a35; } .authorbox img{ border-radius: 4px; }</style><h2 id="tldr">TL;DR:</h2><ol><li>Pooling (and attention) help improve learning ability and positional invariance of BiLSTMs.</li><li>Pooling helps improve sample efficiency (low-resource settings) and is particularly beneficial when important words lie away from the end of the sentence.</li><li>Our proposed pooling technique, max-attention (MaxAtt), helps improve upon past approaches on standard accuracy metrics, and is more robust to distribution shift.</li></ol><h2 id="motivation">Motivation</h2><p>Various pooling techniques, like mean-pooling, max-pooling, and attention<b>*</b>, have shown to improve the performance of RNNs <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552">on</a> <a href="https://arxiv.org/abs/1606.01781">text</a> <a href="https://arxiv.org/abs/1606.01781">classification</a> <a href="static/file/16HLT-hierarchical-attention-networks.pdf">tasks</a>. Despite widespread adoption, precisely <strong>why</strong> and <strong>when</strong> pooling benefits the models is largely unexamined.</p><p>In this work, we identify two key factors that explain the performance benefits of pooling techniques: <strong>learnability</strong>, and <strong>positional invariance</strong>. We examine three commonly used pooling techniques (mean-pooling, max-pooling, and attention), and <strong>propose max-attention</strong>, a novel variant that effectively captures interactions among predictive tokens in a sentence.</p><p><b>*</b> <strong>Attention</strong> aggregates representations via a weighted sum, thus we consider it under the umbrella of pooling in this work.</p><h2 id="overview-of-pooling-and-attention">Overview of Pooling and Attention</h2><p align="center"> <img src="static/picture/overall_figure.png" alt="Pooling Overview" style="width: 1000px;"></p><p>Let $s = {x_1, x_2, \ldots, x_n}$ be an input sentence, where $x_t$ is a representation of the input word at position $t$. A recurrent neural network such as an LSTM produces a hidden state $h_t$, and a cell state $c_t$ for each input word $x_t$, where $h_{t}, c_{t} = \phi(h_{t-1}, c_{t-1}, x_{t})$.</p><p>Standard BiLSTMs concatenate the first hidden state of the backward LSTM, and the last hidden state of the forward LSTM for the final sentence representation: $s_{\text{emb}} = [\overrightarrow{h_n}, \overleftarrow{h_1}]$.</p><p>Pooling produces a sentence embedding that aggregates all hidden states at every word position $t$ (row-wise) using $max$ or $mean$ operation (Figure to the left). Alternately, attention (<a href="https://arxiv.org/abs/1508.04025">Luong attention</a>) aggregates a weighted sum of each hidden state by first multiplying them by a query vector to calculate their importance (Figure to the right). In text classification tasks, a fixed query vector or a <strong>global query</strong> (for the entire corpus) is used to compute the importance value of a token in any sentence.</p><p>The sentence embedding ($s_{\text{emb}}$) is finally fed to a downstream text classifier.</p><h2 id="max-attention">Max-Attention</h2><p>We introduce a novel pooling variant called max-attention (MaxAtt) to capture inter-word dependencies. It uses the max-pooled hidden representation as the query vector for attention. This helps to generate a sentence-specific <strong>local query</strong> vector to calculate attention weights.</p><p align="center"> <img src="static/picture/Untitled.png" alt="max-attention" width="300"></p><p>Formally:</p>\[\begin{aligned} q^{i} &amp;= \max_{t \in (1,n)}(h_{t}^{i}); &amp;\hat{h_{t}} &amp;= h_{t}/\|h_{t}\|\\ \alpha_{t} &amp;= \frac{\exp(\hat{h_{t}}^{\top}q)}{\sum_{j=1}^n\exp(\hat{h_{j}}^{\top}q)}; &amp;s_{\text{emb}} &amp;= \sum_{t=1}^n \alpha_{t}h_{t} \end{aligned}\]<p>Note that the learnable query vector in Luong attention is the same for the entire corpus, whereas in max-attention each sentence has a unique locally-informed query, which we hypothesize helps capture inter-word dependencies better. <a href="static/file/boureau-cvpr-10.pdf">Previous</a> <a href="https://www.aclweb.org/anthology/D17-1070/">literature</a> extensively uses max-pooling to capture the prominent tokens (or objects) in a sentence (or image). Hence, using max-pooled representation as a query for attention allows for a second round of aggregation among important hidden representations.</p><p>Now we present comparisons between pooled and non-pooled BiLSTMs across various axes: their <strong>Gradient Propagation</strong> and <strong>Positional Biases</strong>.</p><h2 id="gradient-propagation">Gradient Propagation</h2><p><i> ~ How does gradient propagation across word positions vary between pooled and non-pooled BiLSTMs? Do gradients vanish for BiLSTMs? ~ </i></p><p>In order to quantify the extent to which the gradients vanish across different word positions, we compute the gradient of the loss function w.r.t the hidden state at every word position <img src="static/picture/t.svg" alt="t">, and study their norm. This is represented by the <img src="static/picture/ell_2.svg" alt="\ell_2"> norm <img src="static/picture/partial h_{t}}.svg" alt="|\frac{\partial L}{\partial h_{t}}|">.</p><p align="center"> <img src="static/picture/vanishing_legend.png" alt="Legend" style="width: 400px;"> <br> <img src="static/picture/vanishing.png" alt="Vanishing with time steps" style="width: 400px;"></p><p>The gradient norm $\ell_2$ norm $\lVert\frac{\partial L}{\partial h_{t}}\rVert$ across different word positions <strong>after training for 500 examples</strong>. BiLSTM_LowF suffers from extreme vanishing gradient, with the gradient norm in the middle nearly $10^{-10}$ times that at the ends.</p><p>The plot suggests that specific initialization of the gates with best practices (such as setting the bias of forget-gate to a high value) helps to reduce the extent of the issue, but the problem still persists. In contrast, none of the pooling techniques face this issue, resulting in an almost straight line.</p><p><i> ~ How does gradient vanishing change as we train our models for more epochs? ~ </i></p><p>We define <strong>Vanishing Ratio</strong>– Given by $\lVert\frac{\partial L}{\partial h_{\text{mid}}}\rVert$ $/$ $\lVert\frac{\partial L}{\partial h_{\text{end}}}\rVert$. It is a measure to quantify the extent of vanishing gradient. Higher values indicate severe vanishing as the gradients reaching the middle are lower than the gradients at the end.</p><p align="center"> <img src="static/picture/ratio_legend.png" alt="Legend" style="width: 700px;"> <br><br> <img src="static/picture/last1_ratios.png" alt="Vanishing Ratios last1" style="width: 350px;"> <img src="static/picture/att_max_ratios.png" alt="Vanishing Ratios att_max" style="width: 350px;"></p><p>The figure above presents vanishing ratios over training steps for BiLSTM and MaxAtt, using 1K, 20K unique training examples from the IMDB dataset. The respective training and validation accuracies are also depicted. We note that the <strong>BiLSTM model overfits on the training data, even before the gates can learn to allow the gradients to pass</strong> through (and mitigate the vanishing gradients problem). Thus, the model prematurely memorizes the training data solely based on the starting and ending few words.</p><p>The vanishing ratio is high for BiLSTM, especially in low-data settings. This results in a 12-14% lower test accuracy compared to other pooling techniques, in the 1K setting. We conclude that the phenomenon of vanishing gradients results in weaker performance of BiLSTM, especially in low training data regimes.</p><p><strong>Key Take-aways:</strong></p><ol><li>Vanishing Gradient is a significant issue for BiLSTMs in the initial few epochs of training.</li><li>On training for more examples, gradient vanishing reduces in BiLSTMs.</li><li>In low-resource setting, this phenomenon leads BiLSTMs to overfit to the training data even before it learnt to propagate gradients to distant states.</li><li>Pooling based methods naturally prevent gradient vanishing by allowing direct propagation of signal to distant hidden states.</li></ol><h2 id="positional-biases">Positional Biases</h2><p>The gradient propagation in BiLSTMs suggests that standard LSTMs should be biased towards the end tokens, as the overall contribution of distant hidden states is extremely low in the gradient of the loss. This implies that the weights of various parameters in an LSTM cell (all cells of an LSTM have tied weights) are barely influenced by the middle words of the sentence.</p><p>We now verify this hypothesis by evaluating positional biases of BiLSTMs with different pooling techniques.</p><h3 id="evaluating-natural-positional-biases">Evaluating Natural Positional Biases</h3><p><i> ~ Can naturally trained recurrent models skip over unimportant words in the begining or the end of the sentence? ~ </i></p><p align="center"> <img src="static/picture/Wiki_Diagram.png" alt="Explain Wiki Setting" width="400"></p><ol><li>We append varying amounts of random Wikipedia sentences to the original data at test time for different models trained over the standard data.</li><li>As the percentage of Wikipedia words added to both ends ↑, the model accuracy ↓ significantly for BiLSTM &amp; mean-pool (Figure to the right below). This suggests that these models are unable to skip over the words at the ends.</li><li>Adding Wikipedia words to just one end (Figure to the left below) does not effect BiLSTM accuracy significantly. This suggests that the BiLSTM is able to draw relevant signal from the other end and make useful predictions.</li></ol><p align="center"> <img src="static/picture/legend.png" alt="Legend" width="500"><br> <img src="static/picture/IMDB_LONG_5K_left.png" alt="Wiki Attack Left" width="250"> <img src="static/picture/IMDB_LONG_5K_mid.png" alt="Wiki Attack Mid" width="250"></p><p align="center"> (a) Wikipedia words added to the right; (b) Wikipedia words added to both ends</p><h3 id="training-to-skip-unimportant-words">Training to Skip Unimportant Words</h3><p><i> ~ How well can different models be trained to skip unrelated words? ~ </i></p><ol><li>We modify the training set to contain input from the modified distribution : Wiki (Left), Wiki (Mid), Wiki (Right).</li><li>BiLSTM accuracy in the Mid setting = majority class baseline in low-resource datasets.</li></ol><p align="center"> <img src="static/picture/Untitled 6.png" alt="IMDB Wiki Table" width="1000"></p><h3 id="fine-grained-positional-biasesbp">Fine-grained Positional Biases&lt;/b&gt;&lt;/p&gt;</h3><p><i> ~ How does the position of a word impact its importance in the final prediction by a model? ~ </i></p><h4 id="the-nwi-metric">The NWI Metric</h4><p align="center"> <img src="static/picture/NWI_Explain.png" alt="NWI Explanation" width="400"></p><ol><li>NWI metric to calculate per-position importance of words.</li><li>For pooled architectures, we observe No bias w.r.t. word position. In case of the Wiki (Mid) setting where only the middle tokens contain original input, the pooled models are able to attribute significantly higher importance to the middle tokens.</li><li>For BiLSTMs there is a huge bias towards the end words <em>even when</em> the original sentence is in the middle.</li><li>Even when sentence length is small, BiLSTMs show bias towards end tokens. Though, they are able to attain non-trivial test accuracies.</li></ol><p><br><br></p><style> .iconbox { /* height: 128px; */ /* padding: 20px; */ } .iconbox > div { text-align: center; /* width: 128px; */ display: inline-block; } .iconbox p { margin: 0; }</style><p align="center"> <img src="static/picture/legend.png" alt="Legend" width="500"></p><center><div class="iconbox"><div id="nwi-1"> <img src="static/picture/YAHOO_LONG_25K_none.png" alt="Yahoo None" style="height: 150px;"><p>Standard</p></div><div id="nwi-2"> <img src="static/picture/YAHOO_LONG_25K_left.png" alt="Yahoo Left" style="height: 150px;"><p>Left</p></div><div id="nwi-3"> <img src="static/picture/YAHOO_LONG_25K_mid.png" alt="Yahoo Mid" style="height: 150px;"><p>Mid</p></div><div id="nwi-4"> <img src="static/picture/YAHOO_SHORT_25K_mid_main.png" alt="Yahoo Mid Short" style="height: 150px;"><p>Short + Mid</p></div></div></center><h2 id="conclusion">Conclusion</h2><p>Through detailed analysis we identify <i><b>why</b></i> and <i><b>when</b></i> pooling representationsare beneficial in RNNs. We attribute the performance benefits of pooling techniques to their <b>learning ability</b> (pooling mitigates the problem of vanishing gradients), and <b>positional invariance</b> (pooling eliminates positional biases).</p><p>Our findings suggest that pooling offers large gains when the <b>training examples are few and long</b>, and <b>salient words lie towards the middle of the sequence</b>.</p><p>Lastly, we introduce a novel pooling technique called <b>max-attention (MaxAtt)</b>, which consistently outperforms other pooling variants, and is robust to addition of unimportant tokens in the text. Most of our insights are derived for sequence classification tasks using RNNs. While the analysis techniques and the pooling variant proposed in the work are general, it remains a part of the future work to evaluate their impact on other tasks and architectures.</p><h2 id="authors">Authors</h2><center><div class="authorbox"><div id="authors-1"> <a href="index1.html"> <img src="static/picture/Profile.jpg" alt="Pratyush" style="height: 100px;"><p>Pratyush Maini</p></a><p style="margin-left: 2.5em;padding: 0 7em 2em 0"></p></div><div id="authors-2"> <a href="https://saikeshav.github.io/"> <img src="static/picture/pp.jpeg" alt="Keshav" style="height: 100px;"><p>Kolluru Sai Keshav</p></a><p style="margin-left: 2.5em;padding: 0 7em 2em 0"></p></div><div id="authors-3"> <a href="https://www.cs.cmu.edu/~ddanish/"> <img src="https://pbs.twimg.com/profile_images/1149339477250379776/73row7EO_400x400.png" alt="Danish" style="height: 100px;"><p>Danish Pruthi</p></a><p style="margin-left: 2.5em;padding: 0 7em 2em 0"></p></div><div id="authors-4"> <a href="http://www.cse.iitd.ac.in/~mausam/"> <img src="static/picture/mausam-head.jpg" alt="Mausam" style="height: 100px;"><p>Mausam</p></a><p style="margin-left: 2.5em;padding: 0 7em 2em 0"></p></div></div></center><h2 id="how-do-i-cite-this-work">How do I cite this work?</h2><p>If you find this work useful, please cite our <a href="https://arxiv.org/abs/2005.00159">paper</a>:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{maini2020pool,
    title = "Why and when should you pool? Analyzing Pooling in Recurrent Architectures",
    author = "Maini, Pratyush and Kolluru, Keshav and Pruthi, Danish and {Mausam}",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.410",
}
</code></pre></div></div></section><footer class="page__meta"><p class="page__taxonomy"> <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong> <span itemprop="keywords"> <a href="https://pratyushmaini.github.io/tags/#max-attention" class="page__taxonomy-item" rel="tag">max-attention</a><span class="sep">, </span> <a href="https://pratyushmaini.github.io/tags/#positional-biases" class="page__taxonomy-item" rel="tag">positional biases</a><span class="sep">, </span> <a href="https://pratyushmaini.github.io/tags/#vanishing-gradients" class="page__taxonomy-item" rel="tag">vanishing gradients</a> </span></p></footer><section class="page__share"><h4 class="page__share-title">Share on</h4><a href="https://twitter.com/intent/tweet?text=https://pratyushmaini.github.io/Pooling-Analysis/" class="btn btn--twitter" title="Share on Twitter"><i class="fab fa-twitter" aria-hidden="true"></i><span> Twitter</span></a> <a href="https://www.facebook.com/sharer/sharer.php?u=https://pratyushmaini.github.io/Pooling-Analysis/" class="btn btn--facebook" title="Share on Facebook"><i class="fab fa-facebook" aria-hidden="true"></i><span> Facebook</span></a> <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://pratyushmaini.github.io/Pooling-Analysis/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fab fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a></section></div></article></div><div class="page__footer"><footer> <!-- start custom footer snippets --> <a href="sitemap.html">Sitemap</a> <!-- end custom footer snippets --><div class="page__footer-follow"><ul class="social-icons"> <!--<li><strong>Follow:</strong></li><li><a href="https://facebook.com/pratyush.maini"><i class="fab fa-facebook-square" aria-hidden="true"></i> Facebook</a></li><li><a href="http://github.com/pratyushmaini"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>--> <!--<li><a href="https://pratyushmaini.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>--></ul></div><div class="page__footer-copyright">&copy; 2024 Pratyush Maini. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div></footer></div><script src="static/js/main.min.js"></script> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script></body></html>
