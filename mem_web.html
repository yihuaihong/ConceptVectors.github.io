<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <!-- <link rel="icon" href="./resources/mars.png" type="image/png"> -->

  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BM130PWZB5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-BM130PWZB5');
  </script>
  <meta charset="utf-8">
  <meta name="description" content="Can Neural Network Memorization Be Localized?">
  <meta name="keywords" content="CLIP, Data Filtering, Multimodal, Datacomp, Vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Can Neural Network Memorization Be Localized?</title>

  <link href="static/css/css-GoogleSans7CNotoSans7CCastoro.css" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index2.css">
  <link rel="stylesheet" href="static/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

  <script src="static/js/jquery.min.js"></script>
  <script defer="" src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index1.js"></script>

  <link rel="icon" href="https://pratyushmaini.github.io/mem_web/favicon.ico?">

</head>
<body>

<section class="hero" style="background-color: #B22234;">
  <div class="hero-body no-bottom-padding">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="color: white;">Can Neural Network Memorization Be Localized?</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block" style="color: white;">
              <a target="_blank" href="index1.html" style="color:blanchedalmond !important;">Pratyush Maini</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
              <a target="_blank" href="https://home.cs.colorado.edu/~mozer/index.php" style="color:blanchedalmond !important;">Michael C. Mozer</a><sup>2</sup>&nbsp;&nbsp;&nbsp;
              <a target="_blank" href="https://haniesedghi.com/" style="color:blanchedalmond !important;">Hanie Sedghi</a><sup>2</sup>&nbsp;&nbsp;&nbsp;
              <a target="_blank" href="https://acmilab.org/" style="color:blanchedalmond !important;">Zachary Lipton</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
              <a target="_blank" href="https://zicokolter.com/" style="color:blanchedalmond !important;">Zico Kolter</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
              <a target="_blank" href="https://pluskid.org/" style="color:blanchedalmond !important;">Chiyuan Zhang</a><sup>2</sup>
              <br><sup>1</sup>Carnegie Mellon University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup>Google&nbsp;&nbsp;
            </span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiV Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2307.09542" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- GitHub Link. -->
              <span class="link-block">
                <a target="_blank" href="https://github.com/pratyushmaini/localizing-memorization" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a target="_blank" href="https://youtu.be/5sRqythe6TE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- twitter Link. -->
              <span class="link-block">
                <a target="_blank" href="https://twitter.com/pratyushmaini/status/1684611469407883264?s=20" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-twitter"></i>
                  </span>
                  <span>Summary</span>
                </a>
              </span>
            </div>

          </div>
          </div>

          </div>
        </div>
      </div>
    
  
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">TLDR</h2>
        <tr>
          <td>
              <h2 class="subtitle">
                <ol>
                <li>We show that <span style="color: red;">memorization</span> is typically <em>not</em> localized to specific model layers, rather is confined to a small fraction of neurons dispersed across the model.
                <li>We propose <span style="color: green;">Example-Tied Dropout</span> that can confine <span style="color: red;">memorization</span> to a pre-defined set of neurons, which can then be thrown away at test time.
              </li></ol>
              </h2>
          </td>
        </tr>
        <tr>
          <td>
            <hr color="gray" size="3">
            <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">#1:  Is <span style="color: red;">Memorization</span> Localized to Final Layers?</h2>
            <p style="font-size: 18px;">
              Past works have suggested that memorization is typically confined to the final few model layers:
              <blockquote>
                <a target="_blank" href="https://arxiv.org/abs/2106.09647" style="color:red !important;">“early layers generalize while later layers memorize” </a>
              <br>
              <a target="_blank" href="https://arxiv.org/abs/2105.14602" style="color:red !important;">“memorization predominantly occurs in deeper layers” </a>
              <br>
              <a target="_blank" href="https://arxiv.org/abs/2105.14602" style="color:red !important;">"generalization can be restored by reverting the final few layer weights”</a>
            </blockquote>
            <p style="font-size: 18px;">
            Our work challenges this belief with three different probes revealing that memorization is dispersed across model layers.
          </p>
            <h3 class="subtitle is-3">Gradient Accounting</h3>
            <div class="content has-text-justified">
              <!-- bullet list -->
              <ul>
                <li><b>Setup:</b> Track the norm of the aggregate gradient of mislabeled & clean examples per layer during training of the model. Average across all epochs.</li>
                <li><b>Observation:</b> 
                  <ol>
                    <li>
                      Even though noisy examples account for only 10% of the dataset, they have a comparable gradient norm to clean ones (90%) 
                      </li>
                      <li>
                      Gradients of clean & noisy examples have extremely -ve cosine similarity throughout training.
                    </li>
                  </ol>
                  
                </li>
              </ul>
              <div class="container">
                <div class="row">
                  <div class="col-6">
                    <img src="static/picture/grad1.png" width="95%" style="border-radius: 10px;">
                    <p style="font-size: 0.8em; text-align: left; padding-left: 1%;"> Gradient norm contribution from noisy examples closely
                      follows that for clean examples even when they constitute only
                      10% of the dataset. Results depicted for epochs 15-20 for ResNet-9 trained on CIFAR-10 with 10% label noise.</p>
                  </div>
                  <div class="col-6">
                    <img src="static/picture/grad2.png" width="100%" style="border-radius: 10px;">
                    <p style="font-size: 0.8em; text-align: left; padding-left: 1%;">Cosine similarity between the average gradients of clean
                      and mislabeled examples per layer, per epoch for ResNet9
                      on CIFAR10 with 10% label noise. The <span style="color: red;">memorization</span>
                      of mislabeled examples happens between epochs 10–30</p>
                  </div>
                </div>
              </div>
            </div>

            <h3 class="subtitle is-3">Layer Rewinding</h3>
            <div class="content has-text-justified">
              <!-- bullet list -->
              <ul>
                <li><b>Setup:</b> 
                  <ol>
                    <li>
                  Checkpoint model after every epoch during training
                  </li> <li>
                  Rewind individual layer weights to an earlier epoch; leave remaining model frozen 
                  </li> <li>
                  Measure accuracy on clean & noisy training points.
                </li></ol>
                <li><b>Observation:</b> 
                  <ol>
                    <li>
                      Rewinding the last layers to an epoch before the noisy examples were learnt (as per the training curve) does not impact the accuracy on them significantly. This suggests that the last layers are typically not critical to the memorization of mislabeled examples. 
                      </li>
                      <li>Critical layers change with the complexity of the dataset & architecture. (see full results on other architectures and datasets in the paper)
                      </li>
              </ol></li></ul>
              <div class="container">
                <div class="row">
                    <img src="static/picture/rewind1.png" width="100%" style="border-radius: 10px;">
                    <p style="font-size: 0.8em; text-align: left; padding-left: 1%;">Change in model accuracy on rewinding individual layers to a previous training epoch for clean examples (left) and mislabeled
                      examples (right). The dataset has 10% random label noise. Epoch 0 represents the model weights at initialization.</p>
                  </div>
              </div>
            </div>
            <h3 class="subtitle is-3">Layer Retraining</h3>
            <div class="content has-text-justified">
              <!-- bullet list -->
              <ul>
                <li><b>Setup:</b> 
                  <ol>
                    <li>
                      Checkpoint model initialization; train to convergence 
                      </li><li>Rewind model weights of individual layers to initialization, keeping rest of the model frozen.
                  </li><li>   Now train the rewound layer on only clean samples.
                  </li><li>  Track accuracy on clean & noisy points while training.
                </li></ol>
                <li><b>Observation:</b> Training on only clean examples confers large accuracy on noisy examples. This confirms that the information required to predict correctly on noisy examples is already contained in other model layers.</li>
                <div class="container">
                  <div class="row">
                    <div class="col-6">
                      <img src="static/picture/retrain1.png" width="100%" style="border-radius: 10px;">
                    </div>
                    <div class="col-6">
                      <img src="static/picture/retrain2.png" width="100%" style="border-radius: 10px;">
                    </div>
                    <p style="font-size: 0.8em; text-align: left; padding-left: 1%;">Layer retraining for CIFAR-10 (left) and MNIST (right). We see that layers 4 and 5 are more important for memorization because all other layers can be trained to 100% accuracy on memorized examples, when only trained on clean examples.</p>
                  </div>
                </div>
              </ul></div>
          </td>
          <td>
          <hr color="gray" size="3">
          <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;"> #2: Localizing <span style="color: red;">Memorization</span> to Fixed Neurons</h2>
          <h3 class="subtitle is-3">Greedy Neuron Removal from a Converged Model</h3>
                <div class="content has-text-justified">
                <!-- bullet list -->
                <ul>
                  <li><b>Setup:</b> 
                      <ol>
                          <li>For a given training example, find the neuron in the model that maximizes the loss on that example if removed but does not increase the average loss on the remaining training samples significantly.</li>
                          <li>Zero-out activations from that neuron. Repeat this process until the prediction is flipped.</li>
                      </ol>
                  </li>
                  <li><b>Observation:</b> 
                      <ol>
                          <li>Memorized examples need fewer neurons to flip the prediction.</li>
                          <li>Upon flipping their prediction, the drop in training set accuracy is much lower.</li>
                          <li>Critical neurons are distributed across layers for both typical & memorized samples.</li>
                      </ol>
                  </li>
              </ul>
              
                </div>
                <div class="row">
                  <div class="col" style="text-align: center">
                    <img src="static/picture/neuron.png" width="100%" style="border-radius:10px; ">
                          <!-- Add table caption in small font add a padding of 10px from left--> 
                    <p style="font-size: 0.8em; text-align: left;padding-left: 1%;">
                      For each example in a subset of 1000 clean and 1000 noisy examples, we iteratively remove the most important neurons from
a ResNet-9 model trained on the CIFAR-10 dataset with 10% random label noise, until the example’s prediction flips.
                    </p>
                  </div>
                </div>
          <h3 class="subtitle is-3">Method: Example-Tied Dropout</h3>
          <div class="content has-text-justified">
            <!-- bullet list -->
            <ul>
              <li><b>Generalization Neurons:</b> A fraction of neurons that always fire and are responsible for generalization.</li>
              <li><b><span style="color: red;">Memorization</span> Neurons:</b> Neurons that only fire for its corresponding (training) example.</li>
              </ul>
              Assign a fixed fraction of neurons to each category during training time. Train until convergence. Drop out all the memorization neurons at test time!
          </div>
          <div class="container">
            <div class="row">
              <div class="col-6">
                <img src="static/picture/drop1.png" width="100%" style="border-radius: 10px;">
                <p style="font-size: 0.8em; text-align: left; padding-left: 1%;">A schematic diagram explaining the difference between the generalization and <span style="color: red;">memorization</span> neurons. At test time, we dropout all the <span style="color: red;">memorization</span> neurons.</p>
              </div>
              <div class="col-6">
                <img src="static/picture/drop2.png" width="70%" style="border-radius: 10px;">
                <p style="font-size: 0.8em; text-align: left; padding-left: 1%;">Forward propagation for input tied to the i<sup>th</sup> <span style="color: red;">memorization</span> neuron. The neuron is activated only when the corresponding input is in the training batch.</p>
              </div>
            </div>
          </div>
          <h3 class="subtitle is-3">Results</h3>
          <div class="content has-text-justified">
            <p>
              Example-Tied Dropout is a simple and effective method to localize <span style="color: red;">memorization</span> in a model. 
            </p>
          </div>
          <div class="row">
            <div style="display: flex; justify-content: center;">
              <img src="static/picture/drop3.png" width="70%" style="border-radius:10px; ">
                  </div>
                    <!-- Add table caption in small font add a padding of 10px from left--> 
              <p style="font-size: 0.8em; text-align: center;padding-left: 1%;justify-content: center;">
                Dropping out <span style="color: red;">memorization</span> neurons leads to a sharp drop
                in accuracy on mislabeled examples with a minor impact on prediction on clean and unseen examples.
              </p>
              <div style="display: flex; justify-content: center;">
              <img src="static/picture/drop4.png" width="50%" style="border-radius:10px; ">
                  </div>
                    <!-- Add table caption in small font add a padding of 10px from left--> 
              <p style="font-size: 0.8em; text-align: center;padding-left: 1%;">Most of the
                clean examples that are
                forgotten when dropping
                out the neurons responsible for <span style="color: red;">memorization</span>
                in the case of Exampletied dropout were either
                mislabeled or inherently
                ambiguous and unique requiring <span style="color: red;">memorization</span> for
                correct classification.</p>
          </div>
          

      </td></tr></div>
    </div>
  

</div> 


</section>



<br>
<br>
<!-- Insert a footer now in CMU red colour -->
<footer class="footer" style="background-color: #B22234;">
  <div class="content has-text-centered">
    <p style="color: white;">
      <bold>CMU</bold> &copy; 2023. All rights reserved.
    </p>
  </div>
</footer>

</body>
</html>