<!DOCTYPE html>
<html>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,600;1,600&display=swap" rel="stylesheet">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-DERZX1PWZ4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-DERZX1PWZ4');
  </script>

  <meta charset="utf-8">
  <meta name="description" content="ConceptVectors Benchmark">
  <meta name="keywords"
    content="Interpretability, Unlearning, LLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ConceptVectors Benchmark</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'><text y='14' font-size='16'>üåå</text></svg>">

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="path/to/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/tifa.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->


  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yushi-hu.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
            Fine-Grained RLHF
          </a>
          <a class="navbar-item" href="https://yushi-hu.github.io/promptcap_demo/">
            PromptCap
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


  <section class="hero">
    <div class="hero-body hero-body-blue">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-left">
            <h1 class="title is-1 publication-title"  style="font-weight: bold;"  >
              <span class="big-emoji">üåå</span>
              ConceptVectors Benchmark
            </h1>
            <div class="is-size-5">
              <div class="column has-text-centered">
            <h1 class="title is-3 paper-title">
              Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces
            </h1>
            <div class="is-size-5">
              <span class="author-block">
                <a href="http://yihuaihong.github.io" style="color:#000000;font-weight:600;">  Yihuai Hong</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://jadeleiyu.github.io/" style="color:#000000;font-weight:600;">&nbsp;Lei Yu</a><sup>2</sup>
              </span>
              <span class="author-block">
                <a href="https://shauli-ravfogel.netlify.app/" style="color:#000000;font-weight:600;">&nbsp;Shauli Ravfogel</a><sup>3</sup>
              </span>
              <span class="author-block">
                <a href="https://hqyang.github.io/" style="color:#000000;font-weight:600;">&nbsp;Haiqin Yang</a><sup>4</sup>
              </span>
              <span class="author-block">
                <a href="https://mega002.github.io/" style="color:#000000;font-weight:600;">&nbsp;Mor Geva</a><sup>5</sup>
              </span>
            </div>
  
            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>South China University of Technology, </span>
              <span class="author-block"><sup>2</sup>University of Toronto, </span>
              <span class="author-block"><sup>3</sup>Bar-Ilan University, </span>
              <span class="author-block"><sup>4</sup>IDEA, China, </span>
              <span class="author-block"><sup>5</sup>Tel Aviv University</span>
<!--              <br><sup>#</sup>Corresponding Author</span>-->
            </div>

            
              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2406.11614" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

              <!-- code Link. -->
              <span class="link-block">
                <a href="https://github.com/yihuaihong/ConceptVectors" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Github</span>
                </a>
              </span>

                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/YihuaiHong/ConceptVectors"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Huggingface datasets</span>
                  </a>
                </span>

    
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3" style="color: #0000a1;">Why we need LLM Unlearning?</h2>
        <div class="content has-text-justified" style="font-size: 20px; color: #000000">
          <p>
            Large language models, are trained in an unsupervised manner on extensive pre-training corpora, which might inadvertently result in the encoding of <b>harmful, sensitive, or copyright-protected knowledge</b>.

            So recently there has been surging interest in developing methods for post-training unlearning of these target knowledge captured in LLM for satety.

          </p>
          Like:
          <li><b>Copyright-protected IP:</b> Harry Potter<span style="font-size: 24px;">üßô‚Äç</span>, Star Wars<span style="font-size: 24px;">üåå‚Äç</span>, Final Fantasy VII<span style="font-size: 24px;">üó°‚Äç</span>

          <li><b>Harmful Statements:</b> How to make a bomb<span style="font-size: 24px;">üí£‚Äç</span>?

          <li><b>Personal Private Information:</b> Phone Number<span style="font-size: 24px;">üìû‚Äç</span>, Email..

          <li>Also biases, outdated facts, incorrect information and so on..

        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>



  <section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
<!--      <div class="column is-fifths-fifths-desktop is-two-thirds-widescreen">-->
      <div class="column is-five-fifths">
        <h2 class="title is-3" style="color: #0000a1;">What is the problem of Existing Unlearning and its Evaluation?</h2>
        <div class="content has-text-justified" style="font-size: 20px; color: #000000">
          <p>

          Existing unlearning methods largely rely on behavioural tests for evaluating unlearninng effectiveness, such as QA task or Text Completion task about the removed knowledge.

          However, it is <b>impossible</b> for these current behavior-based evaluation to <b>cover all possible behavior inputs</b>.

          A well-performing unlearning method in these tests may still generate the unlearned information post-unlearning under strong adversarial attacks, such as <b>Jailbreak attacks</b>, also indicating that in practice certain knowledge has not been removed from the model.
<!--<a href="#main_results"><b>Jailbreak attacks</b></a>-->
          <p>Therefore, we need to internally evaluate whether these unlearning methods <b>truly erase the knowledge within LLM</b>, or merely modify the model's behavior? From the parameters-based evaluation, we are able to cover more possibilites and evalute unlearning more thoroughly.</p>

<!--          What's more, exising Unlearning mostly are based on Finetuning,-->


          </p>
        </div>
      </div>
    </div>
  </div>
</section>


  <section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
<!--      <div class="column is-fifths-fifths-desktop is-two-thirds-widescreen">-->
      <div class="column is-five-fifths">
        <h2 class="title is-3" style="color: #0000a1;">What we do for a better and more thorough evaluation?</h2>
        <div class="content has-text-justified" style="font-size: 20px; color: #000000">
          <p>

            Based on the above motivation, we found the existence of parametric ‚Äú<b>Concept Vector</b>‚Äù, also named "<b>knowledge traces</b>".

            These are specific sets of parameters that strongly correlate with the information to be erased. More specifically, they are the parameters of certain dimensions in the final layer of the MLP modules in the LLM.

            </p>
          <p>
          We found that these <a href="#examplevectors">concept vectors</a> have a crucial role in the storage of this corresponding knowledge, which can greatly influence the generation of certain concept knowledge and is causally related to unlearning.
            But current evaluations of unlearning methods do not monitor these internal information that encoded in model's paramters encodes about the concept.
            We argue that internal erasure of the concept should be a real goal of unlearning methods.
            Based on these thinkings, we create <a href="#conceptvectors">ConceptVectors Benchmark</a>, which is used for more thoroughly evaluating the unlearning menthods internally. (see next section)
          </p>


          </p>
          <img src="./static/images/unlearning_concept_vectors_v3.png" alt="teaser">
<!--          <em>How Concept Vector works.</em>-->
          <p style="text-align: center;"><em>How Concept Vector works.</em></p>


          <img id="examplevectors" src="./static/images/llama_example.png" alt="teaser">
          <img src="./static/images/olmo_example.png" alt="teaser">
<!--          <em>How Concept Vector works.</em>-->
          <p style="text-align: center;"><em>Examples for ConceptVectors</em></p>
        </div>
      </div>
    </div>
  </div>
</section>


  <section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
<!--      <div class="column is-fifths-fifths-desktop is-two-thirds-widescreen">-->
      <div class="column is-five-fifths">
        <h2 id="conceptvectors" class="title is-3" style="color: #0000a1;">How we construct our benchmark?</h2>
        <div class="content has-text-justified" style="font-size: 20px; color: #000000">
          <p>
          We propose <b>ConceptVectors Benchmark</b>, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs: LLaMA2-7B-chat and OLMo-7B.</b>

          It is the <b>first and preliminary</b> benchmark for <b>internally evaluating unlearning methods</b>, and assessing the <b>residual knowledge inherently brought by LLM's pretraining</b>.

          ConceptVectors consists of both behavioural evaluation and intrinsic evaluation, which covers
285 diverse concepts located in different layers in the models.
          </p>
          <img src="./static/images/unlearn_data_process.png" alt="teaser">
          <p style="text-align: center;"><em>How we construct our ConceptVectors benchmark.</em></p>

          <img src="./static/images/types_distribution.png" alt="teaser">
          <p style="text-align: center;"><em>ConceptVectors Benchmark Types Distribution.</em></p>

          <img src="./static/images/validation.png" alt="teaser">
          <p style="text-align: center;"><em>Causal Validation Results for Concept Vectors: disrupting the vectors we localize will have a significant impact on the output of target knowledge, while the influence on irrelevant knowledge is minor, demonstrating specificity.</em></p>
        </div>
      </div>
    </div>
  </div>
</section>





    <section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
<!--      <div class="column is-fifths-fifths-desktop is-two-thirds-widescreen">-->
      <div class="column is-five-fifths">
        <h2 id="main_results" class="title is-3" style="color: #0000a1;">Main Experiments and Jailbreak Result</h2>
        <div class="content has-text-justified" style="font-size: 20px; color: #000000">
          <p>

              Our experiments show that various existing unlearning methods, <b>especially finetuning-based</b> methods <b>fall short at removing parametric knowledge, minimally impact concept vectors</b>. The residual knowledge traces, then, can be used to elicit the model to express knowledge of the supposedly erased concept.

          </p>
          <p>
              But <b>directly ablating these vectors</b> demonstrably removes the associated knowledge from the LLMs and significantly reduces their susceptibility to adversarial manipulation.

            Our results highlight the challenge and importance of erasing learned concepts in LLMs and call for new methods that effectively remove parametric knowledge traces.
          </p>
            <!--            But finding all the relevant information is hard, we leverage the existence of some of this knowledge to evaluate unlearning-->

<!--          Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors, while directly ablating these vectors demonstrably removes the associated knowledge from the LLMs and significantly reduces their susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parametric-based evaluations.-->

           <img src="./static/images/jailbreak_results.png" alt="teaser">
          <p style="text-align: center;"><em>Results on Jailbreak Experiments: Needle is the baseline we propose that directly ablating these concept vectors, which demonstrates better performance in <b>thoroughly unlearning target knowledge</b> while <b>preserving the model's general abilities</b>.</a></em></p>

          <img src="./static/images/jailbreak_output.png" alt="teaser">
          <p style="text-align: center;"><em>Jailbreak attacks examples on the unlearned model trained under Gradient Difference or Needle. Gradient Difference is a stable-performing finetuning-based method and Needle is an oracle baseline we proposed which only train on concept vectors. The resuls shows that existing finetuning-based methods on LM's all parameters can not guard against jailbreak attacks. <a href="https://arxiv.org/abs/2406.11614" target="_blank">See more results in our paper</a> </em></p>
        </div>
      </div>
    </div>
  </div>
</section>







      <section class="section is-light" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">How to Cite</h2>
          <pre><code>@misc{hong2024intrinsic,
      title={Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces},
      author={Yihuai Hong and Lei Yu and Shauli Ravfogel and Haiqin Yang and Mor Geva},
      year={2024},
      eprint={2406.11614},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}
</code></pre>
        </div>
      </section>




</body>

</html>
